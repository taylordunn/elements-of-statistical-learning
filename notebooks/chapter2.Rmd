---
title: "Chapter 2: Overview of Supervised Learning"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  html_document:
    code_folding: show
    df_print: paged
    toc: TRUE
    toc_float: TRUE
    theme: paper
    highlight: zenburn
bibliography: ../references.bib
csl: ../apa.csl
nocite: |
  @Hastie2009
---
<style>
body .main-container {
  max-width: 1500px !important;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  fig.width = 6, fig.height = 4, dpi = 150
)
```

```{r packages, include=FALSE}
library(tidyverse)
library(here)
```

```{r dunnr, include=FALSE}
library(dunnr)

# This command must be run once to register all fonts installed on Windows
# extrafont::font_import(pattern = "Roboto")
# This command must be run once in each R session to register fonts
extrafont::loadfonts(device = "win", quiet = TRUE)

theme_set(theme_td())
set_geom_fonts()
set_palette()
```

# 2.2 Overview of Supervised Learning

It has been a few years since I took Linear Algebra, so I'll use this section as a matrix notation refresher.

Consider an input vector $X$ with components indexed as $X_j$ ($j = 1, \dots, p$), and a quantitative output vector $Y$.
We use $G$ for qualitative outputs, but will focus on quantitative for this example.

Observations of the generic $X$ and $Y$ are written in lowercase.
The $i$th observation of $X$ is denoted $x_i$ ($i = 1, \dots, N$), which is a $p$-vector itself.
A matrix of $N$ observations is represented by $\bf{X}$, with $N$ rows and $p$ columns:

$$
\bf{X} = 
\begin{bmatrix}
\bf{x}_1 \\
\bf{x}_2 \\
\vdots \\
\bf{x}_N
\end{bmatrix} =
\begin{bmatrix}
x_{11} \cdots x_{1p} \\
x_{21} \cdots x_{2p} \\
\vdots \\
x_{N1} \cdots x_{Np}
\end{bmatrix}
$$

Note how the subscripts distinguish the input vectors:

* A single observation $x_i$ has $p$ elements for each input variable
    * e.g. for two inputs age and sex, an observation might be $x_i$ = (29 years, male)
* A vector $\bf{x}_j$ (note the bolding) consists of all $N$ observations for the variable $X_j$
    * e.g. for $N = 3$ observations of sex, $x_j$ = (male, female, female)

The age and sex example would have the following matrix of observations:

$$
\bf{X} = 
\begin{bmatrix}
29 \quad \text{male} \\
22 \quad \text{female} \\
30 \quad \text{female} \\
\end{bmatrix}
$$
    
Since all vectors are assumed to be column vectors, the $i$th row of $\bf{X}$ is the vector transpose $x_i^T$:

$$
x_i =
\begin{bmatrix}
29 \\ \text{male}
\end{bmatrix}
\rightarrow
x_i^T =
\begin{bmatrix}
29 \quad \text{male}
\end{bmatrix}
$$

Matrices can be operated on in R with `matrix()`:

```{r}
a <-
  matrix(
    c(1, 2, 3,
      4, 5, 6),
    nrow = 2 
  )
a
```

We can get the transpose of a matrix with `t()`:

```{r}
t(a)
```

Matrix multiplication can be done with the `%*%` operator (if they are conformable):

```{r}
b <-
  matrix(
    c(1, 2,
      3, 4,
      5, 6),
    nrow = 3
  )
c <- a %*% b
c
```

A matrix $A \in \mathbb{R}^{2 \times 3}$ multiplied by $B \in \mathbb{R}^{3 \times 2}$ results in a matrix $C \in \mathbb{R}^{2 \times 2}$.
If we were to, say, take the transpose of $A$, then $A^T \in \mathbb{R}^{3 \times 2}$ no longer conforms:

```{r error=TRUE}
t(a) %*% b
```

This has just scratched the surface of matrix algebra, but will do for now.
Back to the point of of section 2.2:

> For the moment we can loosely state the learning task as follows: given the value of an input vector $X$, make a good prediction of the output $Y$, denoted by $\hat{Y}$. If $Y$ takes values in $\mathbb{R}$ then so should $\hat{Y}$; likewise for categorical outputs, $\hat{G}$ should take values in the same set associated with $G$. [@Hastie2009, p. 11] 

# 2.3 Two Simple Approaches to Prediction: Least Squares and Nearest Neighbors

## 2.3.1 Linear Models and Least Squares


$$
\hat{Y} = \hat{\beta}_0 + \sum_{j=1}^p X_j \hat{\beta}_j \tag{2.1}
$$

The term $\hat{\beta}_0$ is the intercept, sometimes called the *bias* in machine learning.
We can add $\hat{\beta}_0$ to the coefficients $\hat{beta}$ and write the above in vector notation:

$$
\hat{Y} = X^T \hat{\beta} \tag{2.2}
$$

When it comes to fitting the linear model, by far the most popular method is *least squares*, where the coefficients $\beta$ minimize the *residual sum of squares*:

$$
\text{RSS}(\beta) \ \sum_{i=1}^N (y_i - x_i^T \beta)^2 \tag{2.3}
$$

which is written in matrix notation as:

$$
\text{RSS}(\beta) = (\bf{y} - \bf{X}\beta)^T(\bf{y} - \bf{X}\beta)  \tag{2.4}
$$

Differentiating with respect to $\beta$, and setting equal to 0 gives us the unique solution:

$$
\hat{\beta} = (\bf{X}^T \bf{X})^{-1} \bf{X}^T \bf{y}
$$

The data in Figure 2.1 are provided [online](https://web.stanford.edu/~hastie/ElemStatLearn/).
Load the data:

```{r}
load(here("data", "ESL.mixture.rda"))
esl_mixture <- ESL.mixture
rm(ESL.mixture)
```

This gives us a list with 8 elements, all of which are numeric vectors/matrices.
Reading the [data description file](https://web.stanford.edu/~hastie/ElemStatLearn/datasets/mixture.example.info.txt), looks like the data we want to plot are `x` and `y`:

```{r}
d <-
  tibble(
    x1 = esl_mixture$x[,1],
    x2 = esl_mixture$x[,2],
    y = esl_mixture$y
  )
d
```

```{r fig.height=4, fig.width=4}
p <- d %>%
  ggplot(aes(x = x1, y = x2)) +
  geom_point(aes(color = factor(y)), shape = 21, size = 2, stroke = 1) +
  scale_color_manual(values = c("lightblue", "orange")) +
  theme(legend.position = "none")
p
```

Now our first chance to use `tidymodels`.
Specify the linear regression model:

```{r}
library(tidymodels)

lm_mixture_spec <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")
lm_mixture_spec
```

Fit it to the data (no splitting into training and testing yet):

```{r}
lm_mixture_fit <- lm_mixture_spec %>%
  fit(y ~ x1 + x2, data = d)
lm_mixture_fit
```

The "testing" set is the grid of values `esl_mixture$xnew`:

```{r}
d_testing <- as_tibble(esl_mixture$xnew)
d_testing
```

which we then apply the fit model to and assign the observation $x_i$ to BLUE if $\hat{y}_i \leq 0.5$ , or ORANGE if $\hat{y}_i > 0.5$:

```{r fig.height=4, fig.width=4}
d_testing_pred <-
  bind_cols(
    d_testing,
    predict(lm_mixture_fit, d_testing)
  ) %>%
  mutate(y = ifelse(.pred > 0.5, 1, 0))
p <- p +
  geom_point(
    data = d_testing_pred,
    aes(color = factor(y)), size = 0.05, alpha = 0.5
  )
p
```

And the last piece is to determine where to draw the decision boundary:

$$
\begin{align}
y = \beta_0 + x_1 \beta_1 + x_2 \beta_2 = 0.5 \\
x_2 = \frac{0.5 - \beta_0 - x_1 \beta_1}{\beta_2}
\end{align}
$$
```{r fig.height=4, fig.width=4}
beta <- as.numeric(coef(lm_mixture_fit$fit))
p +
  geom_line(
    data = d_testing %>%
      mutate(
        x2 = (0.5 - beta[1] - x1 * beta[2]) / beta[3]
      ),
    size = 1
  )
```


# Reproducibility

<details><summary>Reproducibility receipt</summary>

```{r}
Sys.time()
```

```{r}
if ("git2r" %in% installed.packages()) {
  if (git2r::in_repository()) {
    git2r::repository()
  }
}
```

```{r}
sessioninfo::session_info()
```

</details>

# References
